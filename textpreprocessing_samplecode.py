# -*- coding: utf-8 -*-
"""TextPreprocessing_SampleCode.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VWZ79ZsVW3SZPWjVxyEUX-rVzZfYNcWt
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install contractions
!pip install emoji
!pip install nltk

import pandas as pd
import numpy as np
from nltk.tokenize import word_tokenize
import statistics
import nltk

nltk.download('punkt')
nltk.download('stopwords')
nltk.download('whitespace')

import nltk
nltk.download('wordnet')

import json
import pandas as pd
import spacy
import nltk
from nltk import word_tokenize
from nltk.corpus import stopwords
import string
import contractions
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
import re
from nltk.tokenize import TweetTokenizer
import emoji
import regex
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
import numpy as np

dft=pd.read_csv("/content/drive/MyDrive/principles of data science/sample_text - Sheet1.csv")
dft

dft['Text_Tokenized'] = dft['text'].str.lower().apply(word_tokenize)
dft

#Preprocessing
ps =PorterStemmer()
lemmatiser = WordNetLemmatizer()
english_stopwords = stopwords.words('english')
exclude = set(string.punctuation)
text=dft['text'].iloc[:40]

# expanded_words = []
# for word in text.split():
#   # using contractions.fix to expand the shortened words
#   expanded_words.append(contractions.fix(word))
#   expanded_text = ' '.join(expanded_words)
# print('Original text: ' + text)
# print('Expanded_text: ' + expanded_text)

def preprocess(text):
  text = contractions.fix(text.lower(), slang=True)
  text=text.lower()
  text= re.sub(r'\d+', '', text)
  text=re.sub(r'$', '', text)
  text=re.sub('<.*?>','',text)
  #text=re.sub(r'\W*\b\w{1,3}\b')
  text=re.sub(r'http\S+', '', text)
  text = text.encode("ascii", "ignore")
  text = text.decode()
  #text=re.sub(r"[\\p{Cf}]", "",text)
  text = ''.join(ch for ch in text if ch not in exclude)
  tokens = word_tokenize(text)
  #print("Tokens:", tokens)
  #text=contractions.fix(tokens)
  tokens = [lemmatiser.lemmatize(t) for t in tokens]
  #tokens=[ps.stem(t) for t in tokens]
  tokens = [t for t in tokens if t not in english_stopwords]
  tokens = [t for t in tokens if len(t) > 2]
  text = " ".join(tokens)
  return text

pre_data=dft['text'].apply(lambda X: preprocess(X))
pre_data

import contractions
# contracted text
text = dft['text'][2]

# creating an empty list
expanded_words = []
for word in text.split():
  # using contractions.fix to expand the shortened words
  expanded_words.append(contractions.fix(word))

expanded_text = ' '.join(expanded_words)
print('Original text: ' + text)
print('Expanded_text: ' + expanded_text)

!pip install demoji

import demoji
demoji.download_codes()

text = dft['text'].apply(lambda X: preprocess(X))
emoji.demojize(text)

